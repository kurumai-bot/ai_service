{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/name/miniconda3/envs/kurumai_service/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import guidance\n",
    "from enum import Enum\n",
    "from guidance import models\n",
    "from pydantic import BaseModel, conlist, TypeAdapter\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import json\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:04<00:00, 16.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/name/miniconda3/envs/kurumai_service/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "chat_template = guidance.chat.Llama3ChatTemplate\n",
    "# lm = models.LlamaCpp(\"../models/meta-llama-3.1-8B-instruct-Q8_0.gguf\", n_gpu_layers=-1, chat_template=chat_template, n_ctx=8192)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "    \"../models/meta-llama-3.1-8B-instruct/\", \n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/meta-llama-3.1-8B-instruct/\")\n",
    "lm = models.Transformers(model=model_8bit, tokenizer=tokenizer, chat_template=chat_template )\n",
    "\n",
    "print(\"model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schema loaded!\n"
     ]
    }
   ],
   "source": [
    "class Emotion(str, Enum):\n",
    "    happy = \"happy\",\n",
    "    sad = \"sad\",\n",
    "    angry = \"angry\",\n",
    "    neutral = \"neutral\"\n",
    "class Response(BaseModel):\n",
    "    reply: str\n",
    "    emotion: Emotion\n",
    "    continue_talking: bool\n",
    "    remaining_responses: int\n",
    "schema = TypeAdapter(conlist(Response, min_length=1))\n",
    "single_schema = Response\n",
    "print(\"schema loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2);  justify-content: center; align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>system</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>You are at a frat party. Split sentences with multiple emotions into multiple responses before asking for a user response. Keep responses short</div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2);  justify-content: center; align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>user</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>Hey there, how are you?</div></div></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@guidance.guidance(stateless=False)\n",
    "def json_gen(lm):\n",
    "    lm += f\"\"\"\\\n",
    "    {{\n",
    "        \"emotion\": \"{guidance.select([e.value for e in Emotion])}\",\n",
    "        \"message\": \"{guidance.gen(stop=['\"'], temperature=1)}\",\n",
    "        \"user_response_needed\": {guidance.select([\"true\", \"false\"], name=\"user_response_needed\")}\n",
    "    }}\"\"\"\n",
    "    return lm\n",
    "\n",
    "@guidance.guidance(stateless=True)\n",
    "def f_gen(lm):\n",
    "    newline = \"\\n\"\n",
    "    dbl_quote = \"\\\"\"\n",
    "    lm += f\"{guidance.gen(stop=[newline, dbl_quote, '<'], temperature=1)}<{guidance.select([e.value for e in Emotion])}>\"\n",
    "    return lm\n",
    "\n",
    "@guidance.guidance(stateless=True)\n",
    "def regex_gen(lm):\n",
    "    emotion_regex = rf\"(.*\\.<({'|'.join([e.value for e in Emotion])})>)+\"\n",
    "    lm += guidance.gen(stop=[\"\\\"\", \"\\n\"], regex=emotion_regex, max_tokens=1000)\n",
    "    return lm\n",
    "\n",
    "state = lm\n",
    "with guidance.system():\n",
    "    state += \"You are at a frat party. Split sentences with multiple emotions into multiple responses before asking for a user response. Keep responses short\"\n",
    "\n",
    "# with guidance.user():\n",
    "#     state += \"tell me 5 facts about paris.\"\n",
    "\n",
    "# with guidance.assistant():\n",
    "#     state += \"\"\"\n",
    "# [\n",
    "#     {\n",
    "#         \"reply\": \"1. The Eiffel Tower is Paris' most iconic landmark and one of the world's most recognizable symbols.\",\n",
    "#         \"emotion\": \"neutral\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"reply\": \"2. The Louvre Museum is home to the Mona Lisa, a famous painting by Leonardo da Vinci, and has a collection of over 550,000 works of art.\",\n",
    "#         \"emotion\": \"neutral\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"reply\": \"3. The Seine River runs through the heart of Paris and offers beautiful views of the city's landmarks and bridges.\",\n",
    "#         \"emotion\": \"neutral\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"reply\": \"4. The Arc de Triomphe honors the soldiers who fought and died for France, and offers stunning views of the city from its top.\",\n",
    "#         \"emotion\": \"neutral\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"reply\": \"5. The Champs-\\\\u00e9lys\\\\u00e9s is one of the world's most famous shopping streets, lined with high-end boutiques, cafes, and restaurants.\",\n",
    "#         \"emotion\": \"neutral\"\n",
    "#     }\n",
    "# ]\n",
    "# \"\"\"\n",
    "\n",
    "with guidance.user():\n",
    "    state += \"Hey there, how are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with guidance.assistant():\n",
    "#     json_schema = state + guidance.json(schema=schema, name=\"response\", temperature=0.5)\n",
    "#     print(json.dumps(json.loads(json_schema[\"response\"]), indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are at a frat party. Split sentences with multiple emotions into multiple responses before asking for a user response. Keep responses short<|eot_id|><|start_header_id|>user<|end_header_id>\n",
      "\n",
      "Hey there, how are you?<|eot_id|>\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are at a frat party. Split sentences with multiple emotions into multiple responses before asking for a user response. Keep responses short<|eot_id|><|start_header_id|>user<|end_header_id>\n",
      "\n",
      "Hey there, how are you?<|eot_id|><|start_header_id|>assistant<|end_header_id>\n",
      "\n",
      "{\n",
      "    \"emotion\": \"sad\",\n",
      "    \"message\": \"I'm not really doing great, been stressed about final exams lately.\",\n",
      "    \"user_response_needed\": true\n",
      "}\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are at a frat party. Split sentences with multiple emotions into multiple responses before asking for a user response. Keep responses short<|eot_id|><|start_header_id|>user<|end_header_id>\n",
      "\n",
      "Hey there, how are you?<|eot_id|><|start_header_id|>assistant<|end_header_id>\n",
      "\n",
      "{\n",
      "    \"emotion\": \"sad\",\n",
      "    \"message\": \"I'm not really doing great, been stressed about final exams lately.\",\n",
      "    \"user_response_needed\": true\n",
      "}<|eot_id|><|start_header_id|>user<|end_header_id>\n",
      "\n",
      "a<|eot_id|>\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are at a frat party. Split sentences with multiple emotions into multiple responses before asking for a user response. Keep responses short<|eot_id|><|start_header_id|>user<|end_header_id>\n",
      "\n",
      "Hey there, how are you?<|eot_id|><|start_header_id|>assistant<|end_header_id>\n",
      "\n",
      "{\n",
      "    \"emotion\": \"sad\",\n",
      "    \"message\": \"I'm not really doing great, been stressed about final exams lately.\",\n",
      "    \"user_response_needed\": true\n",
      "}<|eot_id|><|start_header_id|>user<|end_header_id>\n",
      "\n",
      "a<|eot_id|><|start_header_id|>assistant<|end_header_id>\n",
      "\n",
      "{\n",
      "    \"emotion\": \"angry\",\n",
      "    \"message\": \"What do you mean 'a'?! Can't even have a real conversation here_stride rules too hectic\",\n",
      "    \"user_response_needed\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "json_res = state\n",
    "while True:\n",
    "    with guidance.assistant():\n",
    "        json_res = json_res.set(\"user_response_needed\", \"false\")\n",
    "        print(json_res)\n",
    "        while json_res[\"user_response_needed\"] == \"false\":\n",
    "            last = 0\n",
    "            for part in json_res.stream() + guidance.with_temperature(json_gen(), 1):\n",
    "                json_res = part\n",
    "                string_rep = str(part)\n",
    "                string_rep.rfind(\"<|eot_id|>\")\n",
    "                state.chat_template\n",
    "                last = string_rep\n",
    "            print(json_res._current_prompt())\n",
    "    with guidance.user():\n",
    "        inp = input()\n",
    "        if inp == \"q\":\n",
    "            break\n",
    "        else:\n",
    "            json_res += inp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with guidance.assistant():\n",
    "#     state + guidance.one_or_more(f_gen())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kurumai_service",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
